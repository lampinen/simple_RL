{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim # slim is a wrapper that makes building networks easier\n",
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "from collections import deque \n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear problem introduction\n",
    "\n",
    "We'll start with a very simple problem: A linear track with 6 different locations. At each location, the agent will be able to take the action \"left,\" which will move it to the position to the left, or \"right\" which will move it to the right. (If it is at the left end of the track, the \"left\" action will just leave the agent in the same position.) The agent will start at the left-most end of the track, and we'll give it a reward of +1 for reaching the end of track, which we'll consider a terminal state. \n",
    "\n",
    "<img src=\"linear.png\">\n",
    "\n",
    "Clearly the optimal policy is for the agent to move \"Right\" until it reaches the end of the track. In this part of the homework, we'll explore learning this task with a tabular Q-learning system.\n",
    "\n",
    "## Mathematical questions:\n",
    "\n",
    "(There are 16 questions across 5 sections on this homework, some with code chunks interspersed, make sure you answer all of them! Please answer the questions in a separate document.)\n",
    "\n",
    "1\\. Assuming an optimal, completely greedy policy, and a discount factor of gamma = 0.9, calculate the Q-value of each (state, action) pair. \n",
    "\n",
    "2\\. Under the same assumptions, calculate the value of every state (this shouldn't be much work given the last part).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem, random, and tabular Q controller implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_problem(object):\n",
    "    \"\"\"Class implementing the linear problem\"\"\"\n",
    "    def __init__(self, length=6, rewards=[0, 0, 0, 0, 0, 1], max_lifetime=100):\n",
    "       self.min_state = 0 \n",
    "       self.max_state = length - 1\n",
    "       if len(rewards) != length:\n",
    "           raise ValueError(\"The number of rewards does not match the length... Put zeros on the positions where you don't want a reward\")\n",
    "       self.rewards = rewards # assume that the reward depends only on the state you end up in\n",
    "       self.max_lifetime = max_lifetime\n",
    "\n",
    "       self.reset_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns tuple of current state, which in this problem is just position\"\"\"\n",
    "        return (self.x,)\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Resets state variables to initial conditions\"\"\"\n",
    "        self.x = 0\n",
    "\n",
    "    def update_state(self, action):\n",
    "        \"\"\"Updates state, returns reward of this state\"\"\"\n",
    "        if action == \"left\":\n",
    "            if self.x > self.min_state:\n",
    "                self.x -= 1\n",
    "        else: #action == \"right\"\n",
    "            self.x += 1\n",
    "\n",
    "        return self.rewards[self.x]\n",
    "\n",
    "    def terminal(self):\n",
    "        \"\"\"Checks if state is end\"\"\"\n",
    "        return self.x == self.max_state\n",
    "\n",
    "    def run_trial(self, controller, testing=False):\n",
    "        self.reset_state()\n",
    "        total_reward = 0.\n",
    "        for i in range(self.max_lifetime):\n",
    "            this_state = self.get_state()\n",
    "            this_action = controller.choose_action(this_state)\n",
    "            reward = self.update_state(this_action)\n",
    "            total_reward += reward\n",
    "            new_state = self.get_state()\n",
    "\n",
    "            terminal = self.terminal()\n",
    "            if not testing:\n",
    "                controller.update(this_state, this_action, new_state, reward)\n",
    "\n",
    "            if terminal:\n",
    "                break\n",
    "\n",
    "        if testing:\n",
    "            print(\"Ran testing trial with %s controller, achieved a total reward of %.2f in %i steps\" % (controller.name, total_reward, i)) \n",
    "\n",
    "        return total_reward, i\n",
    "\n",
    "    def run_k_trials(self, controller, k):\n",
    "        \"\"\"Runs k trials, using the specified controller. Controller must have\n",
    "           a choose_action(state) method which returns one of \"left\" and\n",
    "           \"right,\" and must have an update(state, action, next state, reward)\n",
    "           method (if training=True).\"\"\"\n",
    "        avg_tr = 0.\n",
    "        avg_time = 0\n",
    "        for i in range(k):\n",
    "            (tr, time) = self.run_trial(controller)\n",
    "            avg_tr += tr\n",
    "            avg_time += time\n",
    "\n",
    "        avg_tr /= k\n",
    "        avg_time /= k\n",
    "        print(\"Ran %i testing trials with %s controller, achieved an average total reward of %.2f in an average of %i steps\" % (k, controller.name, avg_tr, avg_time)) \n",
    "\n",
    "            \n",
    "\n",
    "class random_controller(object):\n",
    "    \"\"\"Random controller/base class for fancier ones.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Random\"\n",
    "        self.testing = False\n",
    "\n",
    "    def set_testing(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = True\n",
    "\n",
    "    def set_training(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = False\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           this method chooses randomly, should be overridden by fancy\n",
    "           controllers.\"\"\"\n",
    "        return np.random.choice([\"left\", \"right\"])\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update policy or whatever, override.\"\"\"\n",
    "        pass\n",
    "\n",
    "class linear_tabular_Q_controller(random_controller):\n",
    "    \"\"\"Tabular Q-learning controller for the linear problem.\"\"\"\n",
    "    def __init__(self, possible_states=range(6), epsilon=0.05, gamma=0.9, eta=0.1):\n",
    "        \"\"\"Epsilon: exploration probability (epsilon-greedy)\n",
    "           gamma: discount factor\n",
    "           eta: update rate\"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"Tabular Q\"\n",
    "        self.Q_table = {(x,):  {\"left\": 0.01-np.random.rand()/50, \"right\": 0.01-np.random.rand()/50} for x in possible_states} \n",
    "        self.possible_states = possible_states\n",
    "        self.str_possible_states = [str(x) for x in possible_states] # for printing\n",
    "        self.terminal_state = possible_states[-1]\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    " \n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy w.r.t the current Q-table.\"\"\"\n",
    "        if not self.testing and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice([\"left\", \"right\"])\n",
    "        else:\n",
    "            curr_Q_vals = self.Q_table[state]\n",
    "            if curr_Q_vals[\"left\"] > curr_Q_vals[\"right\"]:\n",
    "                return \"left\"\n",
    "            return \"right\"\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update Q table.\"\"\"\n",
    "        if new_state == self.terminal_state:\n",
    "            target = reward \n",
    "        else:\n",
    "            target = reward + self.gamma * max(self.Q_table[new_state].values())\n",
    "\n",
    "        self.Q_table[prev_state][action] = (1 - self.eta) * self.Q_table[prev_state][action] + self.eta * target\n",
    "\n",
    "    def print_pretty_Q_table(self):\n",
    "        \"\"\"Prints a Q-table where the L-R dimension represents state and the\n",
    "           top row represents the Q-value of the \"right\" action, the bottom row\n",
    "           represents the Q-value of the \"left\" action.\"\"\"\n",
    "        print(\"x:\\t\" + \"\\t\".join(self.str_possible_states))\n",
    "        right_Qs = map(lambda x: \"%.2f\" % self.Q_table[(x,)][\"right\"], self.possible_states[:-1])\n",
    "        print(\"right:\\t\"+ \"\\t\".join(right_Qs) + \"\\tend\") \n",
    "        left_Qs = map(lambda x: \"%.2f\" % self.Q_table[(x,)][\"left\"], self.possible_states[:-1])\n",
    "        print(\"left:\\t\"+ \"\\t\".join(left_Qs) + \"\\tend\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem questions\n",
    "\n",
    "(To answer these questions, run both code chunks below, which print the Q tables over different time scales of learning.)\n",
    "\n",
    "3\\. About how long (how many training episodes) does it take the tabular Q-system to converge to the optimal Q values you calculated above?\n",
    "\n",
    "4\\. For which states do the Q-values converge earlier? For which actions? Why? \n",
    "\n",
    "5\\. How does changing epsilon affect this? (Try epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = linear_problem()\n",
    "\n",
    "# create a tabular Q controller \n",
    "np.random.seed(1)\n",
    "tq = linear_tabular_Q_controller(epsilon=0.05)\n",
    "\n",
    "num_train_per_cycle = 20 # how many training episodes to run between tests\n",
    "num_train_cycles = 5 # how many times train/test cycles to run\n",
    "\n",
    "tq.set_testing()\n",
    "lp.run_trial(tq, testing=True)\n",
    "for i in range(num_train_cycles):\n",
    "\n",
    "    tq.set_training()\n",
    "    lp.run_k_trials(tq, num_train_per_cycle)\n",
    "    tq.set_testing()\n",
    "    print(\"After %i training episodes\" % ((i+1) * num_train_per_cycle))\n",
    "    lp.run_trial(tq, testing=True)\n",
    "    print(\"Q-values:\")\n",
    "    tq.print_pretty_Q_table()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create a tabular Q controller \n",
    "np.random.seed(1)\n",
    "tq = linear_tabular_Q_controller(epsilon=0.05)\n",
    "\n",
    "num_train_per_cycle = 500 # how many training episodes to run between tests\n",
    "num_train_cycles = 5 # how many times train/test cycles to run\n",
    "\n",
    "tq.set_testing()\n",
    "lp.run_trial(tq, testing=True)\n",
    "for i in range(num_train_cycles):\n",
    "\n",
    "    tq.set_training()\n",
    "    lp.run_k_trials(tq, num_train_per_cycle)\n",
    "    tq.set_testing()\n",
    "    print(\"After %i training episodes\" % ((i+1) * num_train_per_cycle))\n",
    "    lp.run_trial(tq, testing=True)\n",
    "    print(\"Q-values:\")\n",
    "    tq.print_pretty_Q_table()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Why does the random controller do better than a randomly initialized tabular Q-learner (before learning)? (see code chunk below for a few comparisons):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(5):\n",
    "    # create a random controller and run a trial with it\n",
    "    rc = random_controller()\n",
    "    np.random.seed(seed)\n",
    "    print(\"Random\")\n",
    "    lp.run_trial(rc, testing=True)\n",
    "\n",
    "    # create a tabular Q controller and run a trial with it,\n",
    "    # then run 10000 training trials and run another testing trial\n",
    "    np.random.seed(seed)\n",
    "    tq = linear_tabular_Q_controller()\n",
    "    tq.set_testing()\n",
    "    print(\"Tabular (pre-training)\")\n",
    "    lp.run_trial(tq, testing=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole problem introduction\n",
    "\n",
    "Now we'll explore something a little more interesting: the cartpole problem:\n",
    "\n",
    "<img src=\"cartpole.png\">\n",
    "\n",
    "A pole is attached to a pivot on top of a cart which moves along a one-dimensional track. The goal of the task is to keep the pole balanced (standing upright) by moving the cart side to side. To make this into a MDP like we've discussed, we need the following elements:\n",
    "\n",
    "* *Agent:* the controller of the cart\n",
    "* *Environment:* the cart/world/physics\n",
    "* *State:* we'll define the state to be a tuple of (x position of cart, x velocity of cart, angle of pole, angular velocity of pole).\n",
    "* *Terminal states:* we'll end the episode when the pole tips too far over (> 15 degrees, in this implementation) or when the cart goes too far to either side (> 2.5 units).\n",
    "* *Actions:* to keep it simple, we'll have only two actions: apply a force of +F toward the right, or -F toward the left, which we'll call \"right\" and \"left,\" respectively.\n",
    "* *Rewards:* To keep things simple and clear, we'll only give a reward in terminal states. Since all terminal states are losing, the reward will be -1.\n",
    "\n",
    "We'll compare two Q-learning approaches to this task in this homework: \n",
    "\n",
    "* *Tabular:* \"standard\" Q-learning\n",
    "* *DQN:* A deep-Q network that approximates the Q-function, loosely inspired by the Atari game playing paper.\n",
    "\n",
    "We'll also compare to a baseline controller that takes random actions at every step.\n",
    "\n",
    "Some of the code chunks in this part of the document have been run for you already since they take a non-trivial amount of time (especially the DQN training). However, we still encourage you to play around with the code and get your hands dirty! (Even the DQN training chunk should only take 10-15 minutes on a modern system.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual questions\n",
    "\n",
    "7\\. Since the reward for every *episode* (not every action!) will be -1, why would a Q-learning system learn any interesting behavior on this task?\n",
    "\n",
    "8\\. Why might a DQN (or some other function approximator) be an appropriate choice here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole problem and random controller implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartpole_problem(object):\n",
    "    \"\"\"Class implementing the cartpole world -- you may want to glance at the\n",
    "       methods to see if you can understand what's going on.\"\"\"\n",
    "    def __init__(self, max_lifetime=1000):\n",
    "        self.delta_t = 0.05\n",
    "        self.gravity = 9.8\n",
    "        self.force = 1.\n",
    "        self.cart_mass = 1.\n",
    "        self.pole_mass = 0.2\n",
    "        self.mass = self.cart_mass + self.pole_mass\n",
    "        self.pole_half_length = 1.\n",
    "        self.max_lifetime = max_lifetime\n",
    "\n",
    "        self.reset_state()\n",
    "\n",
    "        # animation constants\n",
    "        self.cart_half_width = 0.25\n",
    "        self.cart_height = 0.2\n",
    "        self.pole_half_width = 0.025\n",
    "        self.cart_wheel_radius = 0.05\n",
    "        self.pole_offset = self.cart_height + 2 * self.cart_wheel_radius - self.pole_half_width \n",
    "        self.cart_wheel_offset = self.cart_half_width - self.cart_wheel_radius\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns current state as a tuple\"\"\"\n",
    "        return (self.x, self.x_dot, self.phi, self.phi_dot)\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset state variables to initial conditions\"\"\"\n",
    "        self.x = 0.\n",
    "        self.x_dot = 0.\n",
    "        self.phi = 0.\n",
    "        self.phi_dot = 0.\n",
    "\n",
    "    def tick(self, action):\n",
    "        \"\"\"Time step according to EOM and action.\"\"\"\n",
    "\n",
    "        if action == \"left\":\n",
    "            action_force = self.force\n",
    "        else:\n",
    "            action_force = -self.force\n",
    "\n",
    "        dt = self.delta_t\n",
    "        self.x += dt * self.x_dot \n",
    "        self.phi += dt * self.phi_dot \n",
    "\n",
    "        sin_phi = np.sin(self.phi)\n",
    "        cos_phi = np.cos(self.phi)\n",
    "\n",
    "        F = action_force + sin_phi * self.pole_mass * self.pole_half_length * (self.phi_dot**2)\n",
    "        phi_2_dot = (sin_phi * self.gravity - cos_phi * F/ self.mass) / (0.5 * self.pole_half_length * (4./3 - self.pole_mass * cos_phi**2 / self.mass))\n",
    "        x_2_dot = (F - self.pole_mass * self.pole_half_length * phi_2_dot) / self.mass \n",
    "        \n",
    "        self.x_dot += dt * x_2_dot \n",
    "        self.phi_dot += dt * phi_2_dot \n",
    "        \n",
    "\n",
    "    def loses(self):\n",
    "        \"\"\"Loses if not within 2.5 m of start and 15 deg. of vertical\"\"\"\n",
    "        return not (-2.5 < self.x < 2.5 and -0.262 < self.phi < 0.262)\n",
    "\n",
    "    def animate(self, trial_state_history, ticks_per_second=20):\n",
    "        \"\"\"Makes a simple video showing the trial\"\"\"\n",
    "        fig, ax = plot.subplots()\n",
    "\n",
    "        ax.set_xlim([-2.5, 2.5])\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax.set_ylim([-1, 3])\n",
    "\n",
    "        # create patches, draw first frame\n",
    "        x, _, phi, _ = trial_state_history[0]\n",
    "\n",
    "        # fg\n",
    "        fg_p = Rectangle((-2.5, -1), 5, 1, facecolor=\"#ccaa99\")\n",
    "        ax.add_patch(fg_p)\n",
    "\n",
    "\n",
    "        # pole\n",
    "        pole_p = Rectangle((x-self.pole_half_width, self.pole_offset), 2*self.pole_half_width, 2*self.pole_half_length, facecolor=\"#777788\")\n",
    "        ax.add_patch(pole_p)\n",
    "        # cart\n",
    "        cart_p = Rectangle((x-self.cart_half_width, 2*self.cart_wheel_radius), 2*self.cart_half_width, self.cart_height, facecolor=\"k\")\n",
    "        ax.add_patch(cart_p)\n",
    "\n",
    "        wheel1_p = Circle((x-self.cart_wheel_offset, self.cart_wheel_radius), self.cart_wheel_radius, facecolor=\"k\")\n",
    "        ax.add_patch(wheel1_p)\n",
    "\n",
    "        wheel2_p = Circle((x+self.cart_wheel_offset, self.cart_wheel_radius), self.cart_wheel_radius, facecolor=\"k\")\n",
    "        ax.add_patch(wheel2_p)\n",
    "\n",
    "        def __draw_frame(state):\n",
    "            x, _, phi, _ = state\n",
    "            pole_p.set_xy((x-self.pole_half_width, self.pole_offset))\n",
    "            pole_p.angle = 57.3*phi # to degrees\n",
    "            cart_p.set_xy((x-self.cart_half_width, 2*self.cart_wheel_radius))\n",
    "            wheel1_p.center = (x-self.cart_wheel_offset, self.cart_wheel_radius)\n",
    "            wheel2_p.center = (x+self.cart_wheel_offset, self.cart_wheel_radius)\n",
    "            if not (-0.262 < phi < 0.262):\n",
    "                pole_p.set_facecolor(\"r\")\n",
    "\n",
    "            \n",
    "        anim = animation.FuncAnimation(fig, __draw_frame,\n",
    "                                       frames=trial_state_history,\n",
    "                                       interval=1000./ticks_per_second,\n",
    "                                       repeat=False)\n",
    "        display(HTML(anim.to_jshtml()))\n",
    "\n",
    "    def run_trial(self, controller, testing=False, animate=False):\n",
    "        self.reset_state()\n",
    "        i = 0\n",
    "        if animate:\n",
    "            trial_state_history = []\n",
    "            trial_state_history.append(self.get_state())\n",
    "        while i < self.max_lifetime:\n",
    "            i += 1\n",
    "            this_state = self.get_state()\n",
    "            this_action = controller.choose_action(this_state)\n",
    "            self.tick(this_action)\n",
    "            new_state = self.get_state()\n",
    "\n",
    "            loss = self.loses()\n",
    "            reward = -1. if loss else 0.\n",
    "            if not testing:\n",
    "                controller.update(this_state, this_action, new_state, reward)\n",
    "\n",
    "            if animate:\n",
    "                trial_state_history.append(new_state)\n",
    "\n",
    "            if loss:\n",
    "                break\n",
    "\n",
    "        if testing:\n",
    "            print(\"Ran testing trial with %s Controller, achieved a lifetime of %i steps\" % (controller.name, i))\n",
    "\n",
    "        if animate:\n",
    "            self.animate(trial_state_history)\n",
    "\n",
    "        return i\n",
    "\n",
    "    def run_k_trials(self, controller, k):\n",
    "        \"\"\"Runs k trials, using the specified controller. Controller must have\n",
    "           a choose_action(state) method which returns one of \"left\" and\n",
    "           \"right,\" and must have an update(state, action, next state, reward)\n",
    "           method (if training=True).\"\"\"\n",
    "        avg_lifetime = 0.\n",
    "        for i in range(k):\n",
    "            avg_lifetime += self.run_trial(controller)\n",
    "\n",
    "        avg_lifetime /= k\n",
    "        print(\"Ran %i trials with %s Controller, (average lifetime of %f steps)\" % (k,  controller.name, avg_lifetime))\n",
    "        \n",
    "cpp = cartpole_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_controller(object):\n",
    "    \"\"\"Random controller/base class for fancier ones.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Random\"\n",
    "        self.testing = False\n",
    "\n",
    "    def set_testing(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = True\n",
    "\n",
    "    def set_training(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = False\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           this method chooses randomly, should be overridden by fancy\n",
    "           controllers.\"\"\"\n",
    "        return np.random.choice([\"left\", \"right\"])\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update policy or whatever, override.\"\"\"\n",
    "        pass\n",
    "    \n",
    "class alternating_controller(object):\n",
    "    \"\"\"Just alternates left and right. Try this out if you think it's a good idea!\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"Alternating\"\n",
    "        self.left = True\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           this method chooses randomly, should be overridden by fancy\n",
    "           controllers.\"\"\"\n",
    "        self.left = not self.left\n",
    "        if self.left:\n",
    "            return \"left\"\n",
    "        else:\n",
    "            return \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a few random controllers with different random seeds\n",
    "# this gives a baseline for comparison\n",
    "for i in range(10):\n",
    "    np.random.seed(i)\n",
    "    cpc = random_controller()\n",
    "    cpp.run_trial(cpc, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and animate one!\n",
    "cpp.run_trial(cpc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the random controller quickly loses control of the pole and lets it tip over.\n",
    "\n",
    "## Tabular Q learning\n",
    "\n",
    "There is a difficulty in making this a tabular Q-learning problem: it's not a finite MDP! Since the space of x values, angles, and velocities is continuous, it's actually infinite. In order to avoid trying to make an infinite table, we'll discretize the space (actually quite drastically), by chopping the position and angle dimensions into 3 bins , and the velocity dimensions into 5, thus reducing the continuous state space to 225 discrete states. It's not perfect by any stretch of the imagination, but as you'll see below, it offers quite an improvement over the random controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class tabular_Q_controller(random_controller):\n",
    "    \"\"\"Tabular Q-learning controller.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05, gamma=0.95, eta=0.1):\n",
    "        \"\"\"Epsilon: exploration probability (epsilon-greedy)\n",
    "           gamma: discount factor\n",
    "           eta: update rate\"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"Tabular Q\"\n",
    "        disc = [-1, 0, 1]\n",
    "        disc_dot = [-2, -1, 0, 1, 2]\n",
    "        self.Q_table = {(x, x_dot, phi, phi_dot): {\"left\": 0.01-np.random.rand()/50, \"right\": 0.01-np.random.rand()/50} for x in disc for x_dot in disc_dot for phi in disc for phi_dot in disc_dot}\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state into discrete with 3 possible values of each\n",
    "           position, 5 possible values of each derivative.\"\"\"\n",
    "        x, x_dot, phi, phi_dot = state\n",
    "        if x > 1.:\n",
    "            x = 1\n",
    "        elif x < -1.:\n",
    "            x = -1\n",
    "        else:\n",
    "            x = 0\n",
    "\n",
    "        if x_dot < -0.1:\n",
    "            x_dot = -2\n",
    "        elif x_dot > 0.1:\n",
    "            x_dot = 2\n",
    "        elif x_dot < -0.03:\n",
    "            x_dot = -1\n",
    "        elif x_dot > 0.03:\n",
    "            x_dot = 1\n",
    "        else:\n",
    "            x_dot = 0\n",
    "\n",
    "        if phi > 0.1:\n",
    "            phi = 1\n",
    "        elif phi < -0.1:\n",
    "            phi = -1\n",
    "        else:\n",
    "            phi = 0\n",
    "\n",
    "        if phi_dot < -0.1:\n",
    "            phi_dot = -2\n",
    "        elif phi_dot > 0.1:\n",
    "            phi_dot = 2\n",
    "        elif phi_dot < -0.03:\n",
    "            phi_dot = -1\n",
    "        elif phi_dot > 0.03:\n",
    "            phi_dot = 1\n",
    "        else:\n",
    "            phi_dot = 0\n",
    "\n",
    "        return (x, x_dot, phi, phi_dot)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy w.r.t the current Q-table.\"\"\"\n",
    "        state = self.discretize_state(state)\n",
    "        if not self.testing and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice([\"left\", \"right\"])\n",
    "        else:\n",
    "            curr_Q_vals = self.Q_table[state]\n",
    "            if curr_Q_vals[\"left\"] > curr_Q_vals[\"right\"]:\n",
    "                return \"left\"\n",
    "            return \"right\"\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update Q table.\"\"\"\n",
    "        prev_state = self.discretize_state(prev_state)\n",
    "        new_state = self.discretize_state(new_state)\n",
    "        if reward != 0.:\n",
    "            target = reward # reward states are terminal in this task\n",
    "        else:\n",
    "            target = self.gamma * max(self.Q_table[new_state].values())\n",
    "\n",
    "        self.Q_table[prev_state][action] = (1 - self.eta) * self.Q_table[prev_state][action] + self.eta * target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller()\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "# for trainable controllers, we'll run a few testing trials during\n",
    "# training to see how they evolve\n",
    "for step in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the tabular Q system gets the balance pretty well, but is unable to keep the car within bounds while doing it (it tries toward the end, but then the pole tips over...)\n",
    "\n",
    "## Tabular Q-learning questions\n",
    "\n",
    "9\\. The tabular Q-learning system does much better than a random controller, but it still only lives about 5 times as long. What could we do to improve the tabular Q system's performance on this task further? For whatever you propose, how would it affect training? \n",
    "\n",
    "10\\. Try setting gamma = 0.0 (living in the moment). What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(gamma=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. What happens if we set gamma = 1 (living in all moments at once)? Naively, one might expect to get random behavior, since all trials get the same total reward, and gamma = 1 is essentially saying that the total reward is all that matters, not when the reward appears. However, this is not what actually happens. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(gamma=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. What happens if you set epsilon = 1 (random behavior while training)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(epsilon=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. What happens if you set epsilon = 0 (no exploration)? Why does this happen here, and what might be different about other tasks that makes exploration important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(epsilon=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought (no answer necessary): Are the discretization values very important? (The current values were picked by a few quick rounds of trial and error.) If we discretized the space more finely, would we see better results? Is it better to space the breaks linearly or quadratically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "In some ways, creating the DQN is simpler than creating the tabular Q-learning system. Neural nets can accept continuous input, so we can simply pass the current state to the network without discretizing. We implemented a simple DQN below, with two hidden layers, and a replay buffer that at each time step stores the current experience and samples one of the previous 1000 time steps to replay. (The buffer persists across episodes.)\n",
    "\n",
    "As you'll see below, this system does quite a bit better. In fact, it reaches the time limit at which the cartpole code stops by default (1000 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn_controller(random_controller):\n",
    "    \"\"\"Simple deep-Q network controller -- 4 inputs (one for each state\n",
    "       variable), two hidden layers, two outputs (Q-left, Q-right), and an\n",
    "       optional replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05, gamma=0.95, eta=1e-4, nh1=100, nh2=100, replay_buffer=True):\n",
    "        \"\"\"Epsilon: exploration probability (epsilon-greedy)\n",
    "           gamma: discount factor\n",
    "           eta: learning rate,\n",
    "           nh1: number of hidden units in first hidden layer,\n",
    "           nh2: number of hidden units in second hidden layer,\n",
    "           replay_buffer: whether to use a replay buffer\"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"DQN\"\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if replay_buffer:\n",
    "            self.replay_buffer = deque()\n",
    "            self.replay_buffer_max_size = 1000\n",
    "        else:\n",
    "            self.replay_buffer = None\n",
    "\n",
    "        # network creation\n",
    "        self.input = tf.placeholder(tf.float32, [1, 4])\n",
    "        h1 = slim.layers.fully_connected(self.input, nh1, activation_fn=tf.nn.tanh)\n",
    "        h2 = slim.layers.fully_connected(h1, nh2, activation_fn=tf.nn.tanh)\n",
    "        self.Q_vals = slim.layers.fully_connected(h2, 2, activation_fn=tf.nn.tanh)\n",
    "\n",
    "        # training stuff\n",
    "        self.target =  tf.placeholder(tf.float32, [1, 2])\n",
    "        self.loss = tf.nn.l2_loss(self.Q_vals - self.target)\n",
    "        optimizer = tf.train.AdamOptimizer(self.eta, epsilon=1e-3) # (this is an unrelated epsilon)\n",
    "        self.train = optimizer.minimize(self.loss)\n",
    "\n",
    "        # session and init\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           epsilon-greedy w.r.t current Q-function approx.\"\"\"\n",
    "        if not self.testing and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice([\"left\", \"right\"])\n",
    "        else:\n",
    "            curr_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(state, ndmin=2)})\n",
    "            if curr_Q_vals[0, 0] > curr_Q_vals[0, 1]:\n",
    "                return \"left\"\n",
    "            return \"right\"\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update policy or whatever, override.\"\"\"\n",
    "        if self.replay_buffer is not None:\n",
    "            # put this (S, A, S, R) tuple in buffer\n",
    "            self.replay_buffer.append((prev_state, action, new_state, reward))\n",
    "            rb_len = len(self.replay_buffer)\n",
    "            # pick a random (S, A, S, R) tuple from buffer\n",
    "            (prev_state, action, new_state,reward) = self.replay_buffer[np.random.randint(0, rb_len)]\n",
    "\n",
    "            # remove a memory if getting too full\n",
    "            if rb_len > self.replay_buffer_max_size:\n",
    "                self.replay_buffer.popleft()\n",
    "\n",
    "        if reward != 0.:\n",
    "            target_val = reward # reward states are terminal in this task\n",
    "        else:\n",
    "            new_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(new_state, ndmin=2)})\n",
    "            target_val = self.gamma * np.max(new_Q_vals)\n",
    "\n",
    "        # hacky way to update only the correct Q value: make the target for the\n",
    "        # other its current value\n",
    "        target_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(prev_state, ndmin=2)})\n",
    "        if action == \"left\":\n",
    "            target_Q_vals[0, 0] = target_val\n",
    "        else:\n",
    "            target_Q_vals[0, 1] = target_val\n",
    "\n",
    "        self.sess.run(self.train, feed_dict={self.input: np.array(prev_state, ndmin=2), self.target: target_Q_vals.reshape([1,2])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with DQN Controller, achieved a lifetime of 24 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 18.629000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 23 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.294000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.562000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 19 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.773000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 18 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 38.124000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 44 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 133.583000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 155 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 138.172000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 109 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 295.301000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 1000 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "dqn = dqn_controller(replay_buffer=True)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(8):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    cpp.run_trial(dqn, testing=True)\n",
    "    \n",
    "cpp.run_trial(dqn, testing=True, animate=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"<video controls src=\"dqn_cartpole.mp4\" type=\"video/mp4\" />\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(We have included this animation so you don't need to train the DQN to see it. If it doesn't display in the notebook when you run the above chunk, you should be able to watch the raw .mp4 file which is also included with the homework.) Notice how the DQN solves both problems: it is able to keep the pole balanced, and stop moving towards the left when it gets too close to the edge of the screen.\n",
    "\n",
    "## DQN questions\n",
    "\n",
    "14\\. Why does the DQN take more episodes to train than the tabular Q-learning system? \n",
    "\n",
    "15\\. In my implementation, I used the tanh activation function at the output layer. Why might this be an appropriate choice here? More specifically, what are some activation functions that would probably NOT yield good results at the output layer?\n",
    "\n",
    "16\\. What happens if we turn off the replay buffer? Why might it be important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "dqn = dqn_controller(replay_buffer=False)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(8):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    cpp.run_trial(dqn, testing=True)\n",
    "    \n",
    "cpp.run_trial(dqn, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought: If you gave the DQN the same discretized states that the tabular Q-network gets, would it do any better than the tabular system does? (Try it out if you're curious!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
