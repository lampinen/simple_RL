{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim # slim is a wrapper that makes building networks easier\n",
    "from collections import deque # deques make better replay buffers than lists since\n",
    "                              # adding/removing from either end is O(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole problem introduction\n",
    "\n",
    "In this homework, we'll explore the cartpole problem:\n",
    "\n",
    "<img src=\"cartpole.png\">\n",
    "\n",
    "A pole is balanced on top of a cart which moves along a one-dimensional track. The goal of the task is to keep the pole balanced by moving the cart side to side. To make this into a MDP like we've discussed, we need the following elements:\n",
    "\n",
    "* *Agent:* the controller of the cart\n",
    "* *Environment:* the cart/world/physics\n",
    "* *State:* we'll define the state to be a tuple of (x position of cart, x velocity of cart, angle of pole, angular velocity of pole).\n",
    "* *Terminal states:* we'll end the episode when the pole tips too far over (> 15 degrees, in this implementation) or when the cart goes too far to either side (> 2.5 units).\n",
    "* *Actions:* to keep it simple, we'll have only two actions: apply a force of +F toward the right, or -F toward the left, which we'll call \"right\" and \"left,\" respectively.\n",
    "* *Rewards:* To keep things simple and clear, we'll only give a reward in terminal states. Since all terminal states are losing, the reward will be -1.\n",
    "\n",
    "We'll compare two Q-learning approaches to this task in this homework: \n",
    "\n",
    "* *Tabular:* \"standard\" Q-learning\n",
    "* *DQN:* A deep-Q network that approximates the Q-function, loosely inspired by the Atari game playing paper.\n",
    "\n",
    "We'll also compare to a baseline controller that takes random actions at every step.\n",
    "\n",
    "Most of the code chunks in this document have been run for you already, since some of them (especially the DQN training) take a non-trivial amount of time. However, we encourage you to play around with the code and get your hands dirty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual questions\n",
    "\n",
    "(There are 10 questions across 3 sections on this homework, some with code chunks interspersed, make sure you answer all of them! They can be answered in a separate document or directly in this file, whichever you prefer.)\n",
    "\n",
    "1\\. Since the reward for every *episode* (not every action!) will be -1, why would a Q-learning system learn any interesting behavior on this task?\n",
    "\n",
    "2\\. Why might a DQN (or some other function approximator) be an appropriate choice here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cartpole_problem(object):\n",
    "    \"\"\"Class implementing the cartpole world -- you may want to glance at the\n",
    "       methods to see if you can understand what's going on.\"\"\"\n",
    "    def __init__(self, max_lifetime=1000):\n",
    "        self.delta_t = 0.05\n",
    "        self.gravity = 9.8\n",
    "        self.force = 1.\n",
    "        self.cart_mass = 1.\n",
    "        self.pole_mass = 0.2\n",
    "        self.mass = self.cart_mass + self.pole_mass\n",
    "        self.pole_half_length = 1.\n",
    "        self.max_lifetime = max_lifetime\n",
    "\n",
    "        self.reset_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"Returns current state as a tuple\"\"\"\n",
    "        return (self.x, self.x_dot, self.phi, self.phi_dot)\n",
    "\n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset state variables to initial conditions\"\"\"\n",
    "        self.x = 0.\n",
    "        self.x_dot = 0.\n",
    "        self.phi = 0.\n",
    "        self.phi_dot = 0.\n",
    "\n",
    "    def tick(self, action):\n",
    "        \"\"\"Time step according to EoM and action.\"\"\"\n",
    "\n",
    "        if action == \"left\":\n",
    "            action_force = self.force\n",
    "        else:\n",
    "            action_force = -self.force\n",
    "\n",
    "        dt = self.delta_t\n",
    "        self.x += dt * self.x_dot\n",
    "        self.phi += dt * self.phi_dot\n",
    "\n",
    "        sin_phi = np.sin(self.phi)\n",
    "        cos_phi = np.cos(self.phi)\n",
    "\n",
    "        F = action_force + sin_phi * self.pole_mass * self.pole_half_length * (self.phi_dot**2)\n",
    "        phi_2_dot = (sin_phi * self.gravity - cos_phi * F/ self.mass) / (0.5 * self.pole_half_length * (4./3 - self.pole_mass * cos_phi**2 / self.mass))\n",
    "        x_2_dot = (F - self.pole_mass * self.pole_half_length * phi_2_dot) / self.mass\n",
    "\n",
    "        self.x_dot += dt * x_2_dot\n",
    "        self.phi_dot += dt * phi_2_dot\n",
    "\n",
    "\n",
    "    def loses(self):\n",
    "        \"\"\"Loses if not within 2.5 units of start and 15 deg. of vertical\"\"\"\n",
    "        return not (-2.5 < self.x < 2.5 and -0.262 < self.phi < 0.262)\n",
    "\n",
    "    def run_trial(self, controller, testing=False):\n",
    "        self.reset_state()\n",
    "        i = 0\n",
    "        while i < self.max_lifetime:\n",
    "            i += 1\n",
    "            this_state = self.get_state()\n",
    "            this_action = controller.choose_action(this_state)\n",
    "            self.tick(this_action)\n",
    "            new_state = self.get_state()\n",
    "\n",
    "            loss = self.loses()\n",
    "            reward = -1. if loss else 0.\n",
    "            if not testing:\n",
    "                controller.update(this_state, this_action, new_state, reward)\n",
    "\n",
    "            if loss:\n",
    "                break\n",
    "\n",
    "        if testing:\n",
    "            print(\"Ran testing trial with %s Controller, achieved a lifetime of %i steps\" % (controller.name, i))\n",
    "\n",
    "        return i\n",
    "\n",
    "\n",
    "    def run_k_trials(self, controller, k):\n",
    "        \"\"\"Runs k trials, using the specified controller. Controller must have\n",
    "           a choose_action(state) method which returns one of \"left\" and\n",
    "           \"right,\" and must have an update(state, action, next state, reward)\n",
    "           method (if training=True).\"\"\"\n",
    "        avg_lifetime = 0.\n",
    "        for i in range(k):\n",
    "            avg_lifetime += self.run_trial(controller)\n",
    "\n",
    "        avg_lifetime /= k\n",
    "        print(\"Ran %i trials with %s Controller, (average lifetime of %f steps)\" % (k,  controller.name, avg_lifetime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_controller(object):\n",
    "    \"\"\"Random controller/base class for fancier ones.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = \"Random\"\n",
    "        self.testing = False\n",
    "\n",
    "    def set_testing(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = True\n",
    "\n",
    "    def set_training(self):\n",
    "        \"\"\"Can toggle exploration, for instance.\"\"\"\n",
    "        self.testing = False\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           this method chooses randomly, should be overridden by fancy\n",
    "           controllers.\"\"\"\n",
    "        return np.random.choice([\"left\", \"right\"])\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update policy or whatever, override.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Random Controller, achieved a lifetime of 16 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 15 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 40 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 18 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 21 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 26 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 33 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 16 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 17 steps\n",
      "Ran testing trial with Random Controller, achieved a lifetime of 14 steps\n"
     ]
    }
   ],
   "source": [
    "cpp = cartpole_problem()\n",
    "\n",
    "# try a few random controllers with different random seeds\n",
    "# this gives a baseline for comparison\n",
    "for i in range(10):\n",
    "    np.random.seed(i)\n",
    "    cpc = random_controller()\n",
    "    cpp.run_trial(cpc, testing=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q learning\n",
    "\n",
    "There is a difficulty in making this a tabular Q-learning problem: it's not a finite MDP! Since the space is continuous, it's actually infinite. In order to avoid trying to make an infinite table, we'll discretize the space (actually quite drastically), by chopping each the position and angle dimensions to only 3 values, and the velocity dimensions to 5, thus reducing the continuous state space to 225 discrete states. It's not perfect, but as you'll see below, it offers quite an improvement over the random controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class tabular_Q_controller(random_controller):\n",
    "    \"\"\"Tabular Q-learning controller.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05, gamma=0.95, eta=0.1):\n",
    "        \"\"\"Epsilon: exploration probability (epsilon-greedy)\n",
    "           gamma: discount factor\n",
    "           eta: update rate\"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"Tabular Q\"\n",
    "        disc = [-1, 0, 1]\n",
    "        disc_dot = [-2, -1, 0, 1, 2]\n",
    "        self.Q_table = {(x, x_dot, phi, phi_dot): {\"left\": 0.01-np.random.rand()/50, \"right\": 0.01-np.random.rand()/50} for x in disc for x_dot in disc_dot for phi in disc for phi_dot in disc_dot}\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        \"\"\"Convert continuous state into discrete with 3 possible values of each\n",
    "           position, 5 possible values of each derivative.\"\"\"\n",
    "        x, x_dot, phi, phi_dot = state\n",
    "        if x > 1.:\n",
    "            x = 1\n",
    "        elif x < -1.:\n",
    "            x = -1\n",
    "        else:\n",
    "            x = 0\n",
    "\n",
    "        if x_dot < -0.1:\n",
    "            x_dot = -2\n",
    "        elif x_dot > 0.1:\n",
    "            x_dot = 2\n",
    "        elif x_dot < -0.03:\n",
    "            x_dot = -1\n",
    "        elif x_dot > 0.03:\n",
    "            x_dot = 1\n",
    "        else:\n",
    "            x_dot = 0\n",
    "\n",
    "        if phi > 0.1:\n",
    "            phi = 1\n",
    "        elif phi < -0.1:\n",
    "            phi = -1\n",
    "        else:\n",
    "            phi = 0\n",
    "\n",
    "        if phi_dot < -0.1:\n",
    "            phi_dot = -2\n",
    "        elif phi_dot > 0.1:\n",
    "            phi_dot = 2\n",
    "        elif phi_dot < -0.03:\n",
    "            phi_dot = -1\n",
    "        elif phi_dot > 0.03:\n",
    "            phi_dot = 1\n",
    "        else:\n",
    "            phi_dot = 0\n",
    "\n",
    "        return (x, x_dot, phi, phi_dot)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Epsilon-greedy w.r.t the current Q-table.\"\"\"\n",
    "        state = self.discretize_state(state)\n",
    "        if not self.testing and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice([\"left\", \"right\"])\n",
    "        else:\n",
    "            curr_Q_vals = self.Q_table[state]\n",
    "            if curr_Q_vals[\"left\"] > curr_Q_vals[\"right\"]:\n",
    "                return \"left\"\n",
    "            return \"right\"\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update Q table.\"\"\"\n",
    "        prev_state = self.discretize_state(prev_state)\n",
    "        new_state = self.discretize_state(new_state)\n",
    "        if reward != 0.:\n",
    "            target = reward # reward states are terminal in this task\n",
    "        else:\n",
    "            target = self.gamma * max(self.Q_table[new_state].values())\n",
    "\n",
    "        self.Q_table[prev_state][action] = (1 - self.eta) * self.Q_table[prev_state][action] + self.eta * target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 80.216000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 123 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 80.714000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 123 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 90.360000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 125 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 72.991000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 78 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 87.945000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller()\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "# for trainable controllers, we'll run a few testing trials during\n",
    "# training to see how they evolve\n",
    "for step in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning questions\n",
    "\n",
    "3\\. The tabular Q-learning system does much better than a random controller, but it still only lives about 5 times as long. What could we do to improve the tabular Q system's performance on this task further? For whatever you propose, how would it affect training? \n",
    "\n",
    "4\\. Try setting gamma = 0.0 (living in the moment). What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 16.086000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 16.200000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 16.151000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 16.020000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 16.090000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(gamma=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. What happens if we set gamma = 1 (living in all moments at once)? Naively, one might expect to get random behavior, since all trials get the same total reward, and gamma = 1 is essentially saying that the total reward is all that matters, not when the reward appears. However, this is not what actually happens. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 68.582000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 43 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 68.383000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 79 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 73.737000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 98 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 81.137000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 123 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 79.202000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 104 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(gamma=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. What happens if you set epsilon = 1 (random behavior while training)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 18.413000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 106 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 18.725000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 106 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 18.308000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 106 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 18.500000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 111 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 18.631000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 111 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(epsilon=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. What happens if you set epsilon = 0 (no exploration)? Why does this happen here, and what might be different about other tasks that makes eexploration important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 97.499000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 126.000000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 126.000000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 126.000000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n",
      "Ran 1000 trials with Tabular Q Controller, (average lifetime of 126.000000 steps)\n",
      "Ran testing trial with Tabular Q Controller, achieved a lifetime of 126 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tqc = tabular_Q_controller(epsilon=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought (no answer necessary): Are the discretization values very important? (The current values were picked by a few quick rounds of trial and error.) If we discretized the space more finely, would we see better results? Is it better to space the breaks linearly or quadratically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n",
    "\n",
    "In some ways, creating the DQN is simpler than creating the tabular Q-learning system. Neural nets can accept continuous input, so we can simply pass the current state to the network without discretizing.\n",
    "\n",
    "As you'll see below, this system does quite a bit better. In fact, it reaches the time limit at which the cartpole code stops by default (1000 steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dqn_controller(random_controller):\n",
    "    \"\"\"Simple deep-Q network controller -- 4 inputs (one for each state\n",
    "       variable), two hidden layers, two outputs (Q-left, Q-right), and an\n",
    "       optional replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, epsilon=0.05, gamma=0.95, eta=1e-4, nh1=100, nh2=100, replay_buffer=True):\n",
    "        \"\"\"Epsilon: exploration probability (epsilon-greedy)\n",
    "           gamma: discount factor\n",
    "           eta: learning rate,\n",
    "           nh1: number of hidden units in first hidden layer,\n",
    "           nh2: number of hidden units in second hidden layer,\n",
    "           replay_buffer: whether to use a replay buffer\"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"DQN\"\n",
    "        self.eta = eta\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        if replay_buffer:\n",
    "            self.replay_buffer = deque()\n",
    "            self.replay_buffer_max_size = 1000\n",
    "        else:\n",
    "            self.replay_buffer = None\n",
    "\n",
    "        # network creation\n",
    "        self.input = tf.placeholder(tf.float32, [1, 4])\n",
    "        h1 = slim.layers.fully_connected(self.input, nh1, activation_fn=tf.nn.tanh)\n",
    "        h2 = slim.layers.fully_connected(h1, nh2, activation_fn=tf.nn.tanh)\n",
    "        self.Q_vals = slim.layers.fully_connected(h2, 2, activation_fn=tf.nn.tanh)\n",
    "\n",
    "        # training stuff\n",
    "        self.target =  tf.placeholder(tf.float32, [1, 2])\n",
    "        self.loss = tf.nn.l2_loss(self.Q_vals - self.target)\n",
    "        optimizer = tf.train.AdamOptimizer(self.eta, epsilon=1e-3) # (this is an unrelated epsilon)\n",
    "        self.train = optimizer.minimize(self.loss)\n",
    "\n",
    "        # session and init\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        \"\"\"Takes a state and returns an action, \"left\" or \"right,\" to take.\n",
    "           epsilon-greedy w.r.t current Q-function approx.\"\"\"\n",
    "        if not self.testing and np.random.rand() < self.epsilon:\n",
    "            return np.random.choice([\"left\", \"right\"])\n",
    "        else:\n",
    "            curr_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(state, ndmin=2)})\n",
    "            if curr_Q_vals[0, 0] > curr_Q_vals[0, 1]:\n",
    "                return \"left\"\n",
    "            return \"right\"\n",
    "\n",
    "    def update(self, prev_state, action, new_state, reward):\n",
    "        \"\"\"Update policy or whatever, override.\"\"\"\n",
    "        if self.replay_buffer is not None:\n",
    "            # put this (S, A, S, R) tuple in buffer\n",
    "            self.replay_buffer.append((prev_state, action, new_state, reward))\n",
    "            rb_len = len(self.replay_buffer)\n",
    "            # pick a random (S, A, S, R) tuple from buffer\n",
    "            (prev_state, action, new_state,reward) = self.replay_buffer[np.random.randint(0, rb_len)]\n",
    "\n",
    "            # remove a memory if getting too full\n",
    "            if rb_len > self.replay_buffer_max_size:\n",
    "                self.replay_buffer.popleft()\n",
    "\n",
    "        if reward != 0.:\n",
    "            target_val = reward # reward states are terminal in this task\n",
    "        else:\n",
    "            new_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(new_state, ndmin=2)})\n",
    "            target_val = self.gamma * np.max(new_Q_vals)\n",
    "\n",
    "        # hacky way to update only the correct Q value: make the target for the\n",
    "        # other its current value\n",
    "        target_Q_vals = self.sess.run(self.Q_vals, feed_dict={self.input: np.array(prev_state, ndmin=2)})\n",
    "        if action == \"left\":\n",
    "            target_Q_vals[0, 0] = target_val\n",
    "        else:\n",
    "            target_Q_vals[0, 1] = target_val\n",
    "\n",
    "        self.sess.run(self.train, feed_dict={self.input: np.array(prev_state, ndmin=2), self.target: target_Q_vals.reshape([1,2])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with DQN Controller, achieved a lifetime of 24 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 18.629000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 23 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.294000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 16 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.562000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 19 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.773000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 18 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 38.124000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 44 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 133.583000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 155 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 138.172000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 109 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 295.301000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 1000 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "dqn = dqn_controller(replay_buffer=True)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(8):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    cpp.run_trial(dqn, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN questions\n",
    "\n",
    "8\\. Why does the DQN take longer to learn than the tabular Q-learning system? (There are a number of potentially correct answers here.)\n",
    "\n",
    "9\\. In my implementation, I used the tanh activation function. Why might this be an appropriate choice here? More specifically, what are some activation functions that would NOT yield good results at the output layer?\n",
    "\n",
    "10\\. What happens if we turn off the replay buffer? Why might it be important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran testing trial with DQN Controller, achieved a lifetime of 51 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 17.409000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.705000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 47 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 20.104000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.570000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 17 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 18.803000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 18.665000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.819000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 17 steps\n",
      "Ran 1000 trials with DQN Controller, (average lifetime of 19.313000 steps)\n",
      "Ran testing trial with DQN Controller, achieved a lifetime of 24 steps\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "dqn = dqn_controller(replay_buffer=False)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(8):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    cpp.run_trial(dqn, testing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought: If you gave the DQN the same discretized states that the tabular Q-network gets, would it do any better than the tabular system does? (Try it out if you're curious!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
