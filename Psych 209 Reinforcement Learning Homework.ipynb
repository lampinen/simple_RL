{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim # slim is a wrapper that makes building networks easier\n",
    "import matplotlib.pyplot as plot\n",
    "from matplotlib import animation\n",
    "from matplotlib.patches import Rectangle, Circle\n",
    "from collections import deque \n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear problem introduction\n",
    "\n",
    "We'll start with a very simple problem: a linear track with 6 different locations. At each location, the agent will be able to take the action \"left,\" which will move it to the position to the left, or \"right\" which will move it to the right. (If it is at the left end of the track, the \"left\" action will just leave the agent in the same position.) The agent will start at the left-most end of the track, and we'll give it a reward of +1 for reaching the end of track, which we'll consider a terminal state. \n",
    "\n",
    "<img src=\"linear.png\">\n",
    "\n",
    "Clearly the optimal policy is for the agent to move \"Right\" until it reaches the end of the track. In this part of the homework, we'll explore learning this task with a tabular Q-learning system.\n",
    "\n",
    "## Mathematical questions:\n",
    "\n",
    "(There are 16 questions across 5 sections on this homework, some with code chunks interspersed, make sure you answer all of them! Please answer the questions in a separate document. We have provided one in .docx format for your convenience, but if you prefer another format you may use your own.)\n",
    "\n",
    "1\\. Assuming an optimal, completely greedy policy, and a discount factor of gamma = 0.9, calculate the Q-value of each (state, action) pair. \n",
    "\n",
    "2\\. Under the same assumptions, calculate the value of every state (this shouldn't be much work given the last part)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem, random, and tabular Q controller implementations\n",
    "\n",
    "Note: The code for the problems and controllers in this homework is separated out into modules, to avoid cluttering the document. However, we encourage you to take a look at the code if you're curious or want to make sure you understand what is going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem questions\n",
    "\n",
    "To answer these questions, run the code chunk below, which prints the Q tables at logarthimically spaced steps in the learning process. Make sure to restart your Jupyter Kernel and run all preceding code chunks (Cell->Run all above) before you start.\n",
    "\n",
    "3\\. About how long (how many training episodes) does it take the tabular Q-system to converge to the optimal Q values you calculated above?\n",
    "\n",
    "4\\. For which states do the Q-values converge earlier? For which actions? Why? \n",
    "\n",
    "5\\. How does changing epsilon affect this? (Try editing the code chunk below to set epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episode_spacing = [16, 64, 256, 1024, 4096]\n",
    "\n",
    "lp = linear.linear_problem()\n",
    "\n",
    "# create a tabular Q controller \n",
    "np.random.seed(1)\n",
    "tq = linear.tabular_Q_controller(epsilon=0.05) # you can change epsilon here\n",
    "\n",
    "\n",
    "tq.set_testing()\n",
    "print(\"Initial:\")\n",
    "lp.run_trial(tq, testing=True)\n",
    "print()\n",
    "\n",
    "prev = 0\n",
    "for i in range(len(test_episode_spacing)):\n",
    "    this_cycle_episodes = test_episode_spacing[i] - prev\n",
    "    prev = this_cycle_episodes\n",
    "    \n",
    "    tq.set_training()\n",
    "    lp.run_k_trials(tq, this_cycle_episodes)\n",
    "    tq.set_testing()\n",
    "    print(\"After %i training episodes\" % (test_episode_spacing[i]))\n",
    "    lp.run_trial(tq, testing=True)\n",
    "    print(\"Q-values:\")\n",
    "    tq.print_pretty_Q_table()\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Run the code chunk below, and observe that the random controller performs better than a randomly initialized tabular Q-learner (before learning). Why does this occur?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(5):\n",
    "    # create a random controller and run a trial with it\n",
    "    rc = linear.random_controller()\n",
    "    np.random.seed(seed)\n",
    "    print(\"Random\")\n",
    "    lp.run_trial(rc, testing=True)\n",
    "\n",
    "    # create a tabular Q controller and run a trial with it,\n",
    "    # then run 10000 training trials and run another testing trial\n",
    "    np.random.seed(seed)\n",
    "    tq = linear.tabular_Q_controller()\n",
    "    tq.set_testing()\n",
    "    print(\"Tabular (pre-training)\")\n",
    "    lp.run_trial(tq, testing=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole problem introduction\n",
    "\n",
    "Now we'll explore something a little more interesting: the cartpole problem:\n",
    "\n",
    "<img src=\"cartpole.png\">\n",
    "\n",
    "A pole is attached to a pivot on top of a cart which moves along a one-dimensional track. The goal of the task is to keep the pole balanced (standing upright) by moving the cart side to side. To make this into a MDP like we've discussed, we need the following elements:\n",
    "\n",
    "* *Agent:* the controller of the cart\n",
    "* *Environment:* the cart/world/physics\n",
    "* *State:* we'll define the state to be a tuple of (x position of cart, x velocity of cart, angle of pole, angular velocity of pole).\n",
    "* *Terminal states:* we'll end the episode when the pole tips too far over (> 15 degrees, in this implementation) or when the cart goes too far to either side (> 2.5 units).\n",
    "* *Actions:* to keep it simple, we'll have only two actions: apply a force of +F toward the right, or -F toward the left, which we'll call \"right\" and \"left,\" respectively.\n",
    "* *Rewards:* To keep things simple and clear, we'll only give a reward in terminal states. Since all terminal states are losing, the reward will be -1.\n",
    "\n",
    "We'll compare two Q-learning approaches to this task in this homework: \n",
    "\n",
    "* *Tabular:* \"standard\" Q-learning\n",
    "* *DQN:* A deep-Q network that approximates the Q-function, loosely inspired by the Atari game playing paper.\n",
    "\n",
    "We'll also compare to a baseline controller that takes random actions at every step.\n",
    "\n",
    "Some of the code chunks in this part of the document have been run for you already since they take a non-trivial amount of time (especially the DQN training). However, we still encourage you to play around with the code and get your hands dirty! (Even the DQN training chunk should only take 10-15 minutes on a modern system.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conceptual questions\n",
    "\n",
    "7\\. Since the reward for every *episode* (not every action!) will be -1, why would a Q-learning system learn any interesting behavior on this task?\n",
    "\n",
    "8\\. Why might a DQN (or some other function approximator) be an appropriate choice here? Compare this with the linear track problem, would a DQN be helpful there? \n",
    "\n",
    "Food for thought (no answer necessary): Can you create an MDP on which a function approximator could not provide a benefit over a tabular controller? In this case, interference would probably cause the function approximator to do much worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole problem and random controller implementation\n",
    "\n",
    "Run the code chunks below to try a few random controllers with different random seeds, to get a baseline for comparison, and animate one, to get an intuition for how the tasks looks. (Again, feel free to poke around in the associated module if you're curious.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartpole\n",
    "cpp = cartpole.cartpole_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    np.random.seed(i)\n",
    "    cpc = cartpole.random_controller()\n",
    "    cpp.run_trial(cpc, testing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and animate one!\n",
    "cpp.run_trial(cpc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the random controller quickly loses control of the pole and lets it tip over. (You can run the chunk a few times to see different trajectories for the random controller.)\n",
    "\n",
    "## Tabular Q learning\n",
    "\n",
    "There is a difficulty in making this a tabular Q-learning problem: it's not a finite MDP! Since the space of x values, angles, and velocities is continuous, it's actually uncountably infinite. In order to avoid trying to make an infinite table, we'll discretize the space (actually quite drastically), by chopping the position and angle dimensions into 3 bins , and the velocity dimensions into 5, thus reducing the continuous state space to 225 discrete states. It's not perfect by any stretch of the imagination, but as you'll see below, it offers quite an improvement over the random controller. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = cartpole.tabular_Q_controller()\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "# for trainable controllers, we'll run a few testing trials during\n",
    "# training to see how they evolve\n",
    "for step in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the tabular Q system gets the balance pretty well, but is unable to keep the car within bounds while keeping the pole balanced (it tries to remain in bounds toward the end, but then the pole tips over...)\n",
    "\n",
    "## Tabular Q-learning questions\n",
    "\n",
    "9\\. The tabular Q-learning system does much better than a random controller, but it still only lives about 5 times as long. What could we do to improve the tabular Q system's performance on this task further? For whatever you propose, how would it affect training? \n",
    "\n",
    "10\\. Try setting gamma = 0.0 (living in the moment), by running the next cell. What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = cartpole.tabular_Q_controller(gamma=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. Try setting gamma = 1 (living in all moments at once), by running the next cell. Naively, one might expect to get random behavior, since all trials get the same total reward, and gamma = 1 is essentially saying that the total reward is all that matters, not when the reward appears. However, this is not what actually happens. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = cartpole.tabular_Q_controller(gamma=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Try setting epsilon = 1 (random behavior while training) by running the next cell. What happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = cartpole.tabular_Q_controller(epsilon=1.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. Try setting epsilon = 0 (no exploration), by running the next cell. Why does this happen here, and what might be different about other tasks that makes exploration important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tqc = cartpole.tabular_Q_controller(epsilon=0.)\n",
    "tqc.set_testing()\n",
    "cpp.run_trial(tqc, testing=True)\n",
    "for i in range(5):\n",
    "    tqc.set_training()\n",
    "    cpp.run_k_trials(tqc, 1000)\n",
    "    tqc.set_testing()\n",
    "    cpp.run_trial(tqc, testing=True)\n",
    "    \n",
    "cpp.run_trial(tqc, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought (no answer necessary): Are the discretization values very important? (The current values were picked by a few quick rounds of trial and error.) If we discretized the space more finely, would we see better results? Is it better to space the breaks linearly or quadratically?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "In some ways, creating the DQN is simpler than creating the tabular Q-learning system. Neural nets can accept continuous input, so we can simply pass the current state to the network without discretizing. We implemented a simple DQN below, with two hidden layers (each with 100 hidden units and a Tanh non-linearity), before an output layer (two units, for the respective Q-values of \"left\" and \"right,\" again with a Tanh non-linearity). We implemented a simple replay buffer that at each time step stores the current experience and samples one of the previous 1000 time steps to replay. (The buffer persists across episodes.) The system was trained using the Adam optimizer with a learning rate of 0.0001. By default the training is epsilon greedy, with epsilon = 0.05, and the testing is greedy. As for the tabular and random systems, the chosen action applies a force of a fixed pre-determined magnitude F either toward the right (+F), or toward the left (-F).\n",
    "\n",
    "As you'll see below, this system does quite a bit better. In fact, it reaches the time limit at which the cartpole code stops by default (1000 steps). (However, note there is some cross-platform variation in the number of episodes required to reach this state...)\n",
    "\n",
    "We have provided representative results from running the training of the DQN below, so that you don't need to run the training yourself (which will take about 15 minutes on a reasonably modern machine). We have provided a video of the system trained with the replay buffer -- run the code chunk  containing `HTML(...)` to see it. (If the video doesn't display in the notebook when you run the code chunk, you should be able to watch the raw .mp4 file which is also included with the homework.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = cartpole.dqn_controller(replay_buffer=True, rseed=0)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(10):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    this_lifetime = cpp.run_trial(dqn, testing=True)\n",
    "    if this_lifetime == 1000:\n",
    "        break\n",
    "    \n",
    "cpp.run_trial(dqn, testing=True, animate=True)    \n",
    "cpp.run_k_trials(dqn, 100) # run an extra hundred trials with \n",
    "                           # some randomness (due to exploration)\n",
    "                           # and some learning to evaluate robustness\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results on my machine (so you don't have to run the training)\n",
    "```\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 24 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 18.629000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 23 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 19.294000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 16 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 19.562000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 19 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 19.773000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 18 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 38.124000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 44 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 133.583000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 155 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 138.172000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 109 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 295.301000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 1000 steps\n",
    "\n",
    "Ran 100 trials with DQN Controller, (average lifetime of 1000.000000 steps)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(\"\"\"<video controls src=\"dqn_cartpole.mp4\" type=\"video/mp4\" />\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the DQN solves both problems: it is able to keep the pole balanced, and stop moving towards the left when it gets too close to the edge of the screen.\n",
    "\n",
    "## DQN questions\n",
    "\n",
    "14\\. Why does the DQN take more episodes to train than the tabular Q-learning system? \n",
    "\n",
    "15\\. In my implementation, I used the tanh activation function at the output layer. Why might this be an appropriate choice here? More specifically, what are some activation functions that would probably NOT yield good results at the output layer?\n",
    "\n",
    "16\\. What happens if we turn off the replay buffer? Why might it be important? (See the text below the code chunk for the output of a representative run on my machine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = cartpole.dqn_controller(replay_buffer=False, rseed=0)\n",
    "dqn.set_testing()\n",
    "cpp.run_trial(dqn, testing=True)\n",
    "for i in range(10):\n",
    "    dqn.set_training()\n",
    "    cpp.run_k_trials(dqn, 1000)\n",
    "    dqn.set_testing()\n",
    "    this_lifetime = cpp.run_trial(dqn, testing=True)\n",
    "    if this_lifetime == 1000:\n",
    "        break\n",
    "    \n",
    "cpp.run_trial(dqn, testing=True, animate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results on my machine (so you don't have to run the training)\n",
    "```\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 18.985000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 16 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 19.454000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 19.749000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 17 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 18.036000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 21.084000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 21 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 40.156000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 30 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 113.236000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 156 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 96.128000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 91 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 95.368000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 121 steps\n",
    "Ran 1000 trials with DQN Controller, (average lifetime of 110.364000 steps)\n",
    "Ran testing trial with DQN Controller, achieved a lifetime of 110 steps\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Food for thought (no answer necessary): If you train the DQN without replay for longer, can you get it to converge? Do you think this would scale to more complex tasks? If you gave the DQN the same discretized states that the tabular Q-network gets, would it do any better than the tabular system does? (Try it out if you're curious!)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
